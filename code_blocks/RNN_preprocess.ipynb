{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "tqdm.pandas(tqdm_notebook)\n",
    "data_path = \"/home/dfsnow/rbans/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# Create ekphrasis preprocessor class\n",
    "ekphrasis_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],  # normalize terms\n",
    "    fix_html=True,  # fix HTML tokens  \n",
    "    segmenter=\"english\",  # corpus for word segmentation\n",
    "    corrector=\"english\",  # corpus for spell correction\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # unpack contractions \n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    dicts=[emoticons]  # replace emojis with words\n",
    ")\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling\n",
    "\n",
    "This section is dedicated to ensuring that the sample drawn from Postgres is sufficiently shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shuffled main training data into memory\n",
    "train = pd.read_csv(os.path.join(data_path, \"main_data_sample.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle all the training data to ensure a random distribution\n",
    "train = shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the shuffled data to disk\n",
    "train.to_csv(os.path.join(data_path, \"main_data_shuffled.csv\"), quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "This section is dedicated to preprocessing all of the data using ekphrasis. Doing this in chunks is more efficient, since ekphrasis takes quite awhile to run. After processing all chunks, we concatenate them back together in the command line and load the cleaned data from here on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for starting after the last completed chunk\n",
    "chunksize = 100000\n",
    "shuffled_data = os.path.join(data_path, \"main_data_shuffled.csv\")\n",
    "start_iter = max([int(chunk[0:4]) for chunk in os.listdir(os.path.join(data_path, 'split'))]) + 1\n",
    "start_row = start_iter * chunksize\n",
    "reader = pd.read_csv(\n",
    "    shuffled_data, index_col=0, skiprows=start_row, chunksize=chunksize,\n",
    "    names=[\"index\", \"id\", \"score\", \"body\", \"label\"]\n",
    ")\n",
    "\n",
    "# Load chunk of overall dataframe into memory, process, then write to CSV\n",
    "for i, chunk in enumerate(reader):\n",
    "    chunk[\"body\"] = chunk.body.map(ekphrasis_processor.pre_process_doc)\n",
    "    chunk.to_csv(\n",
    "        os.path.join(data_path, \"split/\" + str(i + start_iter).zfill(4) + \"_preprocessed_chunk.csv\"),\n",
    "        quoting=csv.QUOTE_NONNUMERIC,\n",
    "        header=False, index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(flatten([(ekphrasis_tokenizer(body)) for body in train[1:500].body])).most_common(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of 10 mil posts for SAGE\n",
    "base_posts = train.loc[train.label == 0].body.sample(n=10000000)\n",
    "hate_posts = train.loc[train.label == 1].body.sample(n=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word counts for the hate and non-hate samples, cleanining with ekphrasis\n",
    "base_counts = dict(Counter(flatten([(ekphrasis_processor.pre_process_doc(body)) for body in base_posts])))\n",
    "hate_counts = flatten([(ekphrasis_processor.pre_process_doc(body)) for body in hate_posts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common hate words\n",
    "hate_vocab = [word for word,count in Counter(hate_counts).most_common(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the counts into numerically comparable arrays\n",
    "hate_array = np.array([hate_counts.get(word,0) for word in hate_vocab])\n",
    "base_array = np.array([base_counts.get(word,0) for word in hate_vocab]) + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SAGE algorithm to get the top K words from hate subs\n",
    "mu = np.log(base_array) - np.log(base_array.sum())\n",
    "beta = sage.estimate(hate_array, mu)\n",
    "hate_words = sage.topK(beta, hate_vocab, 10000)\n",
    "print(hate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the hate words to a list\n",
    "with open(os.path.join(data_path, \"hate_words.csv\"), \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for word in hate_words:\n",
    "        writer.writerow([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split hate and nonhate datasets into train, test, and validate\n",
    "train, test = train_test_split(train, test_size=0.2, random_state=2) \n",
    "train, validate = train_test_split(train, test_size=0.2, random_state=2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda_9.0]",
   "language": "python",
   "name": "conda-env-pytorch_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
