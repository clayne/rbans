{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os, math, sys, re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "# Import model and model helper functions\n",
    "sys.path.append(\"..\")\n",
    "import src.fasttext as ft\n",
    "import src.fasttext_utils as ftu\n",
    "\n",
    "data_dir = '../data'  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the following params\n",
    "model = torch.load(os.path.join(data_dir, 'model/NN_fasttext_model.pt.bak'))\n",
    "model.eval()\n",
    "with open(os.path.join(data_dir, 'model/NN_fasttext_data.pkl'), 'rb') as input:\n",
    "    TEXT = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "ekphrasis_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],  # normalize terms\n",
    "    fix_html=True,  # fix HTML tokens  \n",
    "    segmenter=\"english\",  # corpus for word segmentation\n",
    "    corrector=\"english\",  # corpus for spell correction\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # unpack contractions \n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    tokenizer=ftu.reg_tokenize,\n",
    "    dicts=[emoticons]  # replace emojis with words\n",
    ")\n",
    "\n",
    "def predict_from_sentence(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = ftu.generate_bigrams([tok for tok in ekphrasis_processor.pre_process_doc(sentence)])\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.round(torch.sigmoid(model(tensor)))\n",
    "    return prediction.item()\n",
    "\n",
    "\n",
    "def predict_from_batch(sentence):\n",
    "    model.eval()\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in sentence]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.round(torch.sigmoid(model(tensor)))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_from_sentence(model, 'Help')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_applied(start_row):\n",
    "    chunksize = 100000\n",
    "    filename = int(start_row / chunksize)\n",
    "    shuffled_data = os.path.join(data_path, \"main_data_shuffled.csv\")\n",
    "    chunk = pd.read_csv(\n",
    "        shuffled_data, index_col=0, skiprows=start_row, nrows=chunksize,\n",
    "        names=[\"index\", \"id\", \"score\", \"body\", \"label\"],\n",
    "        dtype={\"index\": np.int64, \"id\": str, \"score\": str, \"body\": str, \"label\": int},\n",
    "    )\n",
    "    chunk[\"body\"] = chunk.body.map(ekphrasis_processor.pre_process_doc)\n",
    "    chunk.to_csv(\n",
    "        os.path.join(data_path, \"split/\" + str(filename).zfill(4) + \"_preprocessed_chunk.csv\"),\n",
    "        quoting=csv.QUOTE_NONNUMERIC,\n",
    "        header=False, index=False\n",
    "    )\n",
    "\n",
    "train_len = 229603257\n",
    "completed = [int(chunk[0:4]) * 100000 for chunk in os.listdir(os.path.join(data_path, 'split/'))] \n",
    "all_jobs = list(range(0, train_len, 100000))\n",
    "jobs = set(all_jobs) - set(completed)\n",
    "print(jobs)\n",
    "pool = Pool(cpu_count() - 1)\n",
    "pool.map(read_iter, jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda_9.0]",
   "language": "python",
   "name": "conda-env-pytorch_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
