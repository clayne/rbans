{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import csv\n",
    "import os, math, sys\n",
    "import string\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from itertools import product\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "# Import model and model helper functions\n",
    "sys.path.append(\"..\")\n",
    "import src.fasttext as ft\n",
    "import src.fasttext_utils as ftu\n",
    "from src.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data_dir = '../data'  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Application to Sampled Data\n",
    "\n",
    "This notebook runs our model over our applied data samples. It takes in the preprocessed chunks created by NN_apply_vader, concatenates them, and then runs each preprocessed comment body through the model to get a binary output. It can also run the model on novel example sentences interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model created by NN_train_model\n",
    "model = torch.load(os.path.join(data_dir, 'model/NN_fasttext_model.pt'))\n",
    "model.eval()\n",
    "with open(os.path.join(data_dir, 'model/NN_fasttext_data.pkl'), 'rb') as input:\n",
    "    TEXT = pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor for converting raw sentences to tokenized input data\n",
    "ekphrasis_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],  # normalize terms\n",
    "    fix_html=True,  # fix HTML tokens  \n",
    "    segmenter=\"english\",  # corpus for word segmentation\n",
    "    corrector=\"english\",  # corpus for spell correction\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # unpack contractions \n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    tokenizer=ftu.reg_tokenize,\n",
    "    dicts=[emoticons]  # replace emojis with words\n",
    ")\n",
    "\n",
    "\n",
    "def predict_from_preprocessed(sentence):\n",
    "    tokenized = ast.literal_eval(sentence)\n",
    "    if len(tokenized) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "        tensor = torch.LongTensor(indexed).to(device)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        prediction = torch.round(torch.sigmoid(model(tensor)))\n",
    "        return prediction.item()\n",
    "    \n",
    "    \n",
    "def predict_from_sentence(sentence):\n",
    "    tokenized = ftu.generate_bigrams([tok.lower() for tok in ekphrasis_processor.pre_process_doc(sentence)])\n",
    "    if len(tokenized) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "        tensor = torch.LongTensor(indexed).to(device)\n",
    "        tensor = tensor.unsqueeze(1)\n",
    "        prediction = torch.round(torch.sigmoid(model(tensor)))\n",
    "        return prediction.item()\n",
    "    \n",
    "\n",
    "def predict_from_list(body_list):\n",
    "    tokenized = [ast.literal_eval(i) for i in body_list]\n",
    "    indexed = [[TEXT.vocab.stoi[t] for t in tok] for tok in tokenized]\n",
    "    b = np.zeros([len(indexed),len(max(indexed,key = lambda x: len(x)))])\n",
    "    for i, j in enumerate(indexed):\n",
    "        b[i][0:len(j)] = j\n",
    "    b = np.array(b).transpose()\n",
    "    tensor = torch.LongTensor(b).to(device)\n",
    "    prediction = torch.round(torch.sigmoid(model(tensor)))\n",
    "    return ftu.flatten(prediction.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is hate speech!\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "sent = \"\"\"Ireland as a whole is far too leftist and cucked to leave the EU unfortunately.\"\"\"\n",
    "\n",
    "if predict_from_sentence(sent) == 1.0:\n",
    "    print(\"This is hate speech!\")\n",
    "else:\n",
    "    print(\"This is not hate speech!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to take a list of all files in a month, concatenate them,\n",
    "# run the model, and then output the combined CSV for each month\n",
    "def concat_chunks(filelist):\n",
    "    file_date = filelist[0][-11:-4]\n",
    "    df = pd.concat((pd.read_csv(\n",
    "        file,\n",
    "        names=[\"id\", \"date\", \"author\", \"subreddit\", \"body\", \"sentiment\"],\n",
    "        dtype={\"id\": str, \"date\": str, \"author\": str, \"subreddit\": str, \"body\": str, \"sentiment\": float},\n",
    "        ) for file in filelist)) \n",
    "    print(\"Finished concatenating\", file_date)\n",
    "    df[\"classification\"] = df.body.map(predict_from_preprocessed)\n",
    "#     df[\"classification\"] = predict_from_list(df.body.tolist())\n",
    "    print(\"Finished classifying\", file_date)\n",
    "    df[\"is_hate\"] = (df.classification == 1.0) & (df.sentiment < -0.05).astype(int)\n",
    "    df.drop(columns=[\"body\", \"id\"], inplace=True)\n",
    "    df.to_csv(\n",
    "        os.path.join(data_dir, \"analysis/concat_applied_data_\" + file_date + \".csv\"),\n",
    "        quoting=csv.QUOTE_NONNUMERIC,\n",
    "        header=True, index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2701\n"
     ]
    }
   ],
   "source": [
    "# Defining a list of jobs that need to be finished\n",
    "completed_jobs = os.listdir(os.path.join(data_dir, 'split/'))\n",
    "concat_jobs = os.listdir(os.path.join(data_dir, 'analysis/'))\n",
    "\n",
    "batch = []\n",
    "for year in range(2015, 2020):\n",
    "    for month in range(1, 13):\n",
    "        batch.append([\n",
    "            os.path.join(data_dir, 'split/' + x) for\n",
    "            x in completed_jobs if x[-11:-4] == str(year) + '_' + str(month).zfill(2) and\n",
    "            'concat_applied_data_' + x[-11:-4] + '.csv' not in concat_jobs\n",
    "        ])\n",
    "        \n",
    "batch = [x for x in batch if x != []]\n",
    "print(len(ftu.flatten(batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished concatenating 2016_09\n"
     ]
    }
   ],
   "source": [
    "# Running the actual model and concatenation\n",
    "# Noteably, this cannot be done in parallel since it relies on the \n",
    "# CUDA implementation of the model for speed\n",
    "for b in batch:\n",
    "    concat_chunks(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda_9.0]",
   "language": "python",
   "name": "conda-env-pytorch_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
