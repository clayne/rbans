{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os, math, sys\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "# Import model and model helper functions\n",
    "sys.path.append(\"..\")\n",
    "from src.fasttext_utils import *\n",
    "from src.fasttext import *\n",
    "\n",
    "seed = 2019\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "data_dir = '../data'  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "max_vocab_size = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the structure of the text data\n",
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    preprocessing=generate_bigrams,\n",
    "    tokenize=SocialTokenizer(lowercase=True).tokenize,\n",
    "    lower=True)\n",
    "\n",
    "LABEL = data.Field(\n",
    "    dtype=torch.float,\n",
    "    sequential=False,\n",
    "    use_vocab=False,\n",
    "    pad_token=None, \n",
    "    unk_token=None)\n",
    "\n",
    "nn_fields = [(\"id\", None),\n",
    "              (\"score\", None),\n",
    "              (\"body\", TEXT),\n",
    "              (\"label\", LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting sets into train test and validate + preprocessing and tokenizing\n",
    "train, validate, test = data.TabularDataset.splits(\n",
    "    path=data_dir,\n",
    "    train='test_train.csv',\n",
    "    validation=\"test_validate.csv\",\n",
    "    test='test_test.csv',\n",
    "    format='csv',\n",
    "    skip_header=False, \n",
    "    fields=nn_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch each set for processing via our model\n",
    "train_iter, validate_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, validate, test), batch_size=batch_size,\n",
    "    sort_key=lambda x: len(x.body), device=device,\n",
    "    repeat=False, shuffle=True)\n",
    "\n",
    "# Load pre-trained embeddings from twitter data\n",
    "vec = vocab.Vectors('glove.twitter.27B.100d.txt', os.path.join(data_dir, 'embeddings'))\n",
    "\n",
    "# Build our corpus of vocabulary\n",
    "TEXT.build_vocab(train, validate, max_size=max_vocab_size, vectors=vec, unk_init = torch.Tensor.normal_)\n",
    "with open(os.path.join(data_dir, 'model/NN_fasttext_data.pkl'), 'wb') as output:\n",
    "    pickle.dump(TEXT, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,500,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the following params\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_weights = TEXT.vocab.vectors\n",
    "embedding_dim = 100\n",
    "output_dim = 1\n",
    "padding_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model = FastText(vocab_size, embedding_dim, output_dim, embedding_weights, padding_idx, unk_idx)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "model = model.to(device)\n",
    "loss = loss.to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 5s | Train Loss: 0.135 | Train Acc: 97.21%\n",
      "Epoch: 02 | Epoch Time: 1m 5s | Train Loss: 0.117 | Train Acc: 97.21%\n",
      "Epoch: 03 | Epoch Time: 1m 5s | Train Loss: 0.110 | Train Acc: 97.22%\n",
      "Epoch: 04 | Epoch Time: 1m 5s | Train Loss: 0.103 | Train Acc: 97.27%\n",
      "Epoch: 05 | Epoch Time: 1m 5s | Train Loss: 0.098 | Train Acc: 97.31%\n",
      "Val. Loss: 1.448 |  Val. Acc: 83.62%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time() \n",
    "    \n",
    "    train_loss, train_acc = train_model(model, train_iter, optim, loss, epoch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    torch.save(model, os.path.join(data_dir, 'model/NN_fasttext_model.pt'))\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    \n",
    "validate_loss, validate_acc = evaluate_model(model, validate_iter, loss)\n",
    "print(f'Val. Loss: {validate_loss:.3f} |  Val. Acc: {validate_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda_9.0]",
   "language": "python",
   "name": "conda-env-pytorch_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
