{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os, math, sys, re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext import vocab\n",
    "\n",
    "# Import model and model helper functions\n",
    "sys.path.append(\"..\")\n",
    "import src.fasttext as ft\n",
    "import src.fasttext_utils as ftu\n",
    "\n",
    "seed = 2019\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "data_dir = '../data'  \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "max_vocab_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the structure of the text data\n",
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    preprocessing=ftu.generate_bigrams,\n",
    "    tokenize=ftu.reg_tokenize,\n",
    "    lower=True)\n",
    "\n",
    "LABEL = data.Field(\n",
    "    dtype=torch.float,\n",
    "    sequential=False,\n",
    "    use_vocab=False,\n",
    "    pad_token=None, \n",
    "    unk_token=None)\n",
    "\n",
    "nn_fields = [(\"id\", None),\n",
    "              (\"score\", None),\n",
    "              (\"body\", TEXT),\n",
    "              (\"label\", LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting sets into train test and validate + preprocessing and tokenizing\n",
    "train, validate, test = data.TabularDataset.splits(\n",
    "    path=data_dir,\n",
    "    train='main_train_subsample.csv',\n",
    "    validation=\"main_validate_subsample.csv\",\n",
    "    test='main_test_subsample.csv',\n",
    "    format='csv',\n",
    "    skip_header=False, \n",
    "    fields=nn_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch each set for processing via our model\n",
    "train_iter, validate_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, validate, test), batch_size=batch_size,\n",
    "    sort_key=lambda x: len(x.body), device=device,\n",
    "    repeat=False, shuffle=True)\n",
    "\n",
    "# Load pre-trained embeddings from twitter data\n",
    "vec = vocab.Vectors('glove.twitter.27B.100d.txt', os.path.join(data_dir, 'embeddings'))\n",
    "\n",
    "# Build our corpus of vocabulary\n",
    "TEXT.build_vocab(train, validate, max_size=max_vocab_size, vectors=vec, unk_init = torch.Tensor.normal_)\n",
    "with open(os.path.join(data_dir, 'model/NN_fasttext_data.pkl'), 'wb') as output:\n",
    "    pickle.dump(TEXT, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 5,000,301 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the following params\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_weights = TEXT.vocab.vectors\n",
    "embedding_dim = 100\n",
    "output_dim = 1\n",
    "padding_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model = ft.FastText(vocab_size, embedding_dim, output_dim, embedding_weights, padding_idx, unk_idx)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "model = model.to(device)\n",
    "loss = loss.to(device)\n",
    "\n",
    "print(f'The model has {ftu.count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Idx: 5000, Training Loss: 0.4151, Training Accuracy:  79.69%\n",
      "Epoch: 1, Idx: 10000, Training Loss: 0.3528, Training Accuracy:  84.38%\n",
      "Epoch: 1, Idx: 15000, Training Loss: 0.3549, Training Accuracy:  85.94%\n",
      "Epoch: 1, Idx: 20000, Training Loss: 0.3343, Training Accuracy:  87.50%\n",
      "Epoch: 1, Idx: 25000, Training Loss: 0.3483, Training Accuracy:  85.94%\n",
      "Epoch: 1, Idx: 30000, Training Loss: 0.2381, Training Accuracy:  92.19%\n",
      "Epoch: 1, Idx: 35000, Training Loss: 0.3278, Training Accuracy:  89.06%\n",
      "Epoch: 1, Idx: 40000, Training Loss: 0.2538, Training Accuracy:  93.75%\n",
      "Epoch: 1, Idx: 45000, Training Loss: 0.3203, Training Accuracy:  87.50%\n",
      "Epoch: 1, Idx: 50000, Training Loss: 0.3338, Training Accuracy:  90.62%\n",
      "Epoch: 1, Idx: 55000, Training Loss: 0.4448, Training Accuracy:  85.94%\n",
      "Epoch: 1, Idx: 60000, Training Loss: 0.2710, Training Accuracy:  89.06%\n",
      "Epoch: 1, Idx: 65000, Training Loss: 0.2375, Training Accuracy:  93.75%\n",
      "Epoch: 1, Idx: 70000, Training Loss: 0.3822, Training Accuracy:  84.38%\n",
      "Epoch: 1, Idx: 75000, Training Loss: 0.3532, Training Accuracy:  82.81%\n",
      "Epoch: 01 | Epoch Time: 28m 22s | Train Loss: 0.378 | Train Acc: 84.31%\n",
      "Epoch: 2, Idx: 5000, Training Loss: 0.3175, Training Accuracy:  85.94%\n",
      "Epoch: 2, Idx: 10000, Training Loss: 0.3138, Training Accuracy:  89.06%\n",
      "Epoch: 2, Idx: 15000, Training Loss: 0.4182, Training Accuracy:  82.81%\n",
      "Epoch: 2, Idx: 20000, Training Loss: 0.4295, Training Accuracy:  78.12%\n",
      "Epoch: 2, Idx: 25000, Training Loss: 0.4262, Training Accuracy:  85.94%\n",
      "Epoch: 2, Idx: 30000, Training Loss: 0.4687, Training Accuracy:  76.56%\n",
      "Epoch: 2, Idx: 35000, Training Loss: 0.2849, Training Accuracy:  90.62%\n",
      "Epoch: 2, Idx: 40000, Training Loss: 0.3809, Training Accuracy:  78.12%\n",
      "Epoch: 2, Idx: 45000, Training Loss: 0.4215, Training Accuracy:  82.81%\n",
      "Epoch: 2, Idx: 50000, Training Loss: 0.2511, Training Accuracy:  90.62%\n",
      "Epoch: 2, Idx: 55000, Training Loss: 0.5026, Training Accuracy:  71.88%\n",
      "Epoch: 2, Idx: 60000, Training Loss: 0.2123, Training Accuracy:  93.75%\n",
      "Epoch: 2, Idx: 65000, Training Loss: 0.2186, Training Accuracy:  93.75%\n",
      "Epoch: 2, Idx: 70000, Training Loss: 0.3209, Training Accuracy:  85.94%\n",
      "Epoch: 2, Idx: 75000, Training Loss: 0.3921, Training Accuracy:  82.81%\n",
      "Epoch: 02 | Epoch Time: 27m 54s | Train Loss: 0.376 | Train Acc: 84.35%\n",
      "Epoch: 3, Idx: 5000, Training Loss: 0.3396, Training Accuracy:  85.94%\n",
      "Epoch: 3, Idx: 10000, Training Loss: 0.3481, Training Accuracy:  84.38%\n",
      "Epoch: 3, Idx: 15000, Training Loss: 0.4675, Training Accuracy:  82.81%\n",
      "Epoch: 3, Idx: 20000, Training Loss: 0.2460, Training Accuracy:  95.31%\n",
      "Epoch: 3, Idx: 25000, Training Loss: 0.4116, Training Accuracy:  82.81%\n",
      "Epoch: 3, Idx: 30000, Training Loss: 0.3813, Training Accuracy:  85.94%\n",
      "Epoch: 3, Idx: 35000, Training Loss: 0.3629, Training Accuracy:  84.38%\n",
      "Epoch: 3, Idx: 40000, Training Loss: 0.3896, Training Accuracy:  84.38%\n",
      "Epoch: 3, Idx: 45000, Training Loss: 0.4560, Training Accuracy:  78.12%\n",
      "Epoch: 3, Idx: 50000, Training Loss: 0.3029, Training Accuracy:  89.06%\n",
      "Epoch: 3, Idx: 55000, Training Loss: 0.2927, Training Accuracy:  85.94%\n",
      "Epoch: 3, Idx: 60000, Training Loss: 0.3569, Training Accuracy:  84.38%\n",
      "Epoch: 3, Idx: 65000, Training Loss: 0.5015, Training Accuracy:  76.56%\n",
      "Epoch: 3, Idx: 70000, Training Loss: 0.4220, Training Accuracy:  79.69%\n",
      "Epoch: 3, Idx: 75000, Training Loss: 0.4355, Training Accuracy:  89.06%\n",
      "Epoch: 03 | Epoch Time: 28m 13s | Train Loss: 0.375 | Train Acc: 84.41%\n",
      "Epoch: 4, Idx: 5000, Training Loss: 0.4374, Training Accuracy:  82.81%\n",
      "Epoch: 4, Idx: 10000, Training Loss: 0.4617, Training Accuracy:  84.38%\n",
      "Epoch: 4, Idx: 15000, Training Loss: 0.3549, Training Accuracy:  82.81%\n",
      "Epoch: 4, Idx: 20000, Training Loss: 0.4893, Training Accuracy:  76.56%\n",
      "Epoch: 4, Idx: 25000, Training Loss: 0.3709, Training Accuracy:  84.38%\n",
      "Epoch: 4, Idx: 30000, Training Loss: 0.3347, Training Accuracy:  85.94%\n",
      "Epoch: 4, Idx: 35000, Training Loss: 0.3216, Training Accuracy:  85.94%\n",
      "Epoch: 4, Idx: 40000, Training Loss: 0.3416, Training Accuracy:  85.94%\n",
      "Epoch: 4, Idx: 45000, Training Loss: 0.3805, Training Accuracy:  79.69%\n",
      "Epoch: 4, Idx: 50000, Training Loss: 0.2062, Training Accuracy:  93.75%\n",
      "Epoch: 4, Idx: 55000, Training Loss: 0.5330, Training Accuracy:  76.56%\n",
      "Epoch: 4, Idx: 60000, Training Loss: 0.4499, Training Accuracy:  84.38%\n",
      "Epoch: 4, Idx: 65000, Training Loss: 0.3166, Training Accuracy:  87.50%\n",
      "Epoch: 4, Idx: 70000, Training Loss: 0.2958, Training Accuracy:  85.94%\n",
      "Epoch: 4, Idx: 75000, Training Loss: 0.3061, Training Accuracy:  89.06%\n",
      "Epoch: 04 | Epoch Time: 28m 21s | Train Loss: 0.375 | Train Acc: 84.43%\n",
      "Epoch: 5, Idx: 5000, Training Loss: 0.2817, Training Accuracy:  87.50%\n",
      "Epoch: 5, Idx: 10000, Training Loss: 0.4598, Training Accuracy:  78.12%\n",
      "Epoch: 5, Idx: 15000, Training Loss: 0.4129, Training Accuracy:  84.38%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time() \n",
    "    \n",
    "    train_loss, train_acc = ft.train_model(model, train_iter, optim, loss, epoch)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = ft.epoch_time(start_time, end_time)\n",
    "    torch.save(model, os.path.join(data_dir, 'model/NN_fasttext_model.pt'))\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    \n",
    "validate_loss, validate_acc = ft.evaluate_model(model, validate_iter, loss)\n",
    "print(f'Val. Loss: {validate_loss:.3f} | Val. Acc: {validate_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cd528ce708e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project2/capp30255/dfsnow/rbans/src/fasttext.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, validate_iter, loss_func)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchtext-0.4.0-py3.6.egg/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchtext-0.4.0-py3.6.egg/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchtext-0.4.0-py3.6.egg/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchtext-0.4.0-py3.6.egg/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type list)"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = ft.evaluate_model(model, validate_iter, loss)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda_9.0]",
   "language": "python",
   "name": "conda-env-pytorch_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
