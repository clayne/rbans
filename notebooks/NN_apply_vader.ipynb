{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import csv\n",
    "import os, sys, re\n",
    "import ast\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from itertools import product\n",
    "\n",
    "# Import model and model helper functions\n",
    "sys.path.append(\"..\")\n",
    "import src.fasttext as ft\n",
    "import src.fasttext_utils as ftu\n",
    "from src.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER + Preprocessing for Applied Data\n",
    "\n",
    "This notebook applies sentiment analysis and preprocessing to applied data samples from each month of a 1 year window around our diff-in-diff event. Each month of interest has a sample of 10 million posts, stored in a CSV. This notebook breaks each CSV into 100 chunks (for multiprocessing) and applies two functions:\n",
    "\n",
    "1. VADER sentiment analysis to raw (unprocessed) comment data\n",
    "2. Ekphrasis reprocessing on raw comment data: cleaning, tokenizing, and adding bigrams for later use with our model\n",
    "\n",
    "The resulting chunks are concatenated and then scored with our model in the NN_apply_model notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 1grams ...\n",
      "Reading english - 2grams ...\n",
      "Reading english - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize ekphrasis preprocessing using GitHub defaults + a regex tokenizer\n",
    "ekphrasis_processor = TextPreProcessor(\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'number'],  # normalize terms\n",
    "    fix_html=True,  # fix HTML tokens  \n",
    "    segmenter=\"english\",  # corpus for word segmentation\n",
    "    corrector=\"english\",  # corpus for spell correction\n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # unpack contractions \n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    tokenizer=ftu.reg_tokenize,\n",
    "    dicts=[emoticons]  # replace emojis with words\n",
    ")\n",
    "\n",
    "# Add simple error handling to the VADER analyzer\n",
    "def compound_analyzer(sentence):\n",
    "    try:\n",
    "        return analyzer.polarity_scores(sentence)['compound']\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# Function for converting a raw sentence to preprocessed, tokenized list\n",
    "def model_preprocess(sentence):\n",
    "    tokenized = ftu.generate_bigrams([tok.lower() for tok in ekphrasis_processor.pre_process_doc(sentence)])\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n"
     ]
    }
   ],
   "source": [
    "# This function takes in the name of an output file and creates \n",
    "# that file based on a subsection of the correct input file\n",
    "# It splits input files into chunks and preforms preprocessing in parallel\n",
    "def apply_vader(file):\n",
    "    chunksize = 100000\n",
    "    start_row = int(float(file[0:4]) * chunksize)\n",
    "    if start_row == 0:\n",
    "        start_row = 1\n",
    "    filename = \"applied/\" + str(file[5:])\n",
    "    applied_data = os.path.join(data_dir, filename)\n",
    "    chunk = pd.read_csv(\n",
    "        applied_data, skiprows=start_row, nrows=chunksize,\n",
    "        names=[\"id\", \"date\", \"author\", \"subreddit\", \"body\"],\n",
    "        dtype={\"id\": str, \"date\": str, \"author\": str, \"subreddit\": str, \"body\": str},\n",
    "    )\n",
    "    chunk[\"sentiment\"] = chunk.body.map(compound_analyzer)\n",
    "    chunk[\"body\"] = chunk.body.map(model_preprocess)\n",
    "    chunk.to_csv(\n",
    "        os.path.join(data_dir, \"split/\" + file),\n",
    "        quoting=csv.QUOTE_NONNUMERIC,\n",
    "        header=False, index=False\n",
    "    )\n",
    "\n",
    "# Creating a list of all jobs \n",
    "file_list = os.listdir(os.path.join(data_dir, 'applied/'))\n",
    "chnk_list = [str(x).zfill(4) + \"_\" for x in range(101)]\n",
    "all_jobs = [''.join(x) for x in list(product(chnk_list, file_list))]    \n",
    "\n",
    "completed_jobs = os.listdir(os.path.join(data_dir, 'split/'))\n",
    "\n",
    "jobs = set(all_jobs) - set(completed_jobs)\n",
    "jobs = [x for x in jobs if x[-4:] == \".csv\"]\n",
    "\n",
    "# Running all jobs\n",
    "print(len(jobs))\n",
    "pool = Pool(18)\n",
    "pool.map(apply_vader, jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda_9.0]",
   "language": "python",
   "name": "conda-env-pytorch_cuda_9.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
