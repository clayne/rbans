{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PRE-RNN TOKENIZATION\n",
    "\n",
    "'''\n",
    "This code block takes in our csv data and returns a simple tokenized\n",
    "tensor for use in an RNN model\n",
    "'''\n",
    "    \n",
    "import operator\n",
    "import os, math\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter, defaultdict\n",
    "import copy\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# set the random seeds so the experiments can be replicated exactly\n",
    "seed = 30255\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Global class labels\n",
    "HATE_LABEL = \"hate\"\n",
    "BASE_LABEL = \"nonhate\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "CSV_FILE = \"comments_sample.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "\n",
    "# create list of hate subreddits for parsing\n",
    "HATERS = ['The_Donald', '4chan4trump', 'KotakuInAction', 'CringeAnarchy']\n",
    "\n",
    "# takes in csv and spits out strings for the list\n",
    "def stringer(unfiltered):\n",
    "    \n",
    "    subreddit = unfiltered.subreddit.tolist()\n",
    "    body = unfiltered.body.tolist()\n",
    "    \n",
    "    temp = [str(i) for i in body]\n",
    "    \n",
    "    final = [list(a) for a in zip(temp, subreddit)]\n",
    "    \n",
    "    return final\n",
    "\n",
    "# takes in csv and spits out classified body\n",
    "def findHaters(unfiltered):\n",
    "        \n",
    "    full_body = [item[0] for item in unfiltered]\n",
    "    \n",
    "    filtered = []\n",
    "    hate_body = []\n",
    "    hate_mark = []\n",
    "    \n",
    "    for body, key in unfiltered:\n",
    "        keys = str(key)\n",
    "        if any(keys in h for h in HATERS):\n",
    "            filtered.extend([HATE_LABEL])\n",
    "            hate_body.extend([body])\n",
    "            hate_mark.extend([HATE_LABEL])\n",
    "        else:\n",
    "            filtered.extend([BASE_LABEL])\n",
    "    \n",
    "    tot_final = [list(a) for a in zip([b.split() for b in full_body], filtered)]\n",
    "    hate_final = [list(a) for a in zip([b.split() for b in hate_body], hate_mark)]\n",
    "            \n",
    "    return tot_final, hate_final\n",
    "        \n",
    "f = stringer(df)\n",
    "g,h = findHaters(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "More more advanced tokenization, with lemmatization process\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(corpus):\n",
    "    \n",
    "    body = [item[0] for item in corpus]\n",
    "    classifier = [item[1] for item in corpus]\n",
    "    \n",
    "    midpoint = []\n",
    "    \n",
    "    for s in body:\n",
    "        temp = []\n",
    "        \n",
    "        for w in s:\n",
    "            this = lemma.lemmatize(w)\n",
    "            temp.append(this)\n",
    "        \n",
    "        midpoint.append(temp)\n",
    "    \n",
    "    final = [list(a) for a in zip(midpoint, classifier)]\n",
    "    \n",
    "    return final\n",
    "\n",
    "fin_base = lemmatizer(g)\n",
    "fin_hate = lemmatizer(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAGE IMPLEMENTATION FOR BASELINE\n",
    "\n",
    "'''\n",
    "This code block takes in our tokenized data and returns a dictionary pairing\n",
    "words from the tokenized data and their SAGE score for further use\n",
    " \n",
    "Citation:\n",
    " \n",
    "Eisenstein, Jacob, Amr Ahmed, and Eric P. Xing. \"Sparse Additive Generative Models of Text.\"\n",
    "Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011.\n",
    "'''\n",
    "\n",
    "import sage\n",
    "from collections import Counter\n",
    "\n",
    "# define counter helper function\n",
    "def getCountDict(filename):\n",
    "    with open(filename) as fin:\n",
    "        return {word:int(count) for word,count in [line.rstrip().split() for line in fin.readlines()]}\n",
    "    \n",
    "# counts for hate subreddit\n",
    "hate_counts = getCountDict(fin_hate)\n",
    "\n",
    "# counts for all subreddits in corpus\n",
    "base_counts = getCountDict(fin_base)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "vocab = [word for word,count in Counter(hate_counts).most_common(5000)]\n",
    "\n",
    "x_hate = np.array([hate_counts[word] for word in vocab])\n",
    "x_base = np.array([base_counts[word] for word in vocab]) + 1.\n",
    "\n",
    "# Compute the base log-probabilities of each word\n",
    "mu = np.log(x_base) - np.log(x_base.sum())\n",
    "\n",
    "# Run SAGE\n",
    "eta = sage.estimate(x_hate,mu)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Print words especially frequent in subreddit compared to the baseline\n",
    "print sage.topK(eta,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code block subsets our hate sample based on a list of hate words from SAGE\n",
    " \n",
    "'''\n",
    "\n",
    "# assumes list from SAGE of top 10000\n",
    "# assumes lemmatized data in [[[\"this\", \"format\"], 'hate'], [[\"right\", \"here\"], 'nonhate']]\n",
    "SAGE = hate_list\n",
    "HATE = hate_lemma\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def subsetter(hate_lemma, hate_list):\n",
    "    \n",
    "    subset = []\n",
    "    \n",
    "    for x in hate_lemma:\n",
    "        for body, key in x:\n",
    "            for bit in body:\n",
    "                if bit in hate_list:\n",
    "                    if x not in subset:\n",
    "                        subset.extend(x)\n",
    "    return subset\n",
    "            \n",
    "hate_subset = subsetter(HATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
